head(InsectSprays,15)
sB
summary(InsectSprays[,2])
sapply(InsectSprays)
sapply(InsectSprays, FUN=class())
sapply(InsectSprays, FUN=class(col(InsectSprays)))
sapply(InsectSprays,class)
fit<-lm(count~spray, swiss)
fit<-lm(count~spray, data=InsectSprays)
summary(fit)
summary(fit)$coef
est<-summary(fit)$coef[,1]
mean(InsectSprays$count(sprayA))
mean(InsectSprays$count==sprayA)
mean(InsectSprays$count==A)
mean(InsectSprays$count=="A")
mean(sA)
mean(sB)
nfit<-lm(count~spray-1, InsectSprays)
summary(nfit)
summary(nfit)$coef
spray2<- relevel(spray, "C")
spray2<- relevel(InsectSprays$spray, "C")
fit2<-lm(count~spray, InsectSprays)
fit2<-lm(count~spray2, InsectSprays)
summary(fit2)$coef
mean(sC)
(fit$coef[2]-fit$coef[3])/1.6011
dim(WHO)
dim(hunger)
948
names(hunger)
fit<-lm(X~., hunger)
fit<-lm(Numeric~Year, hunger)
summary(fit)$coef
summary(lm(Numeric~x[hunger$sex=="Female"], hunger))$coef
lmF<-lm(Numeric~Year[hunger$sex=="Female"], hunger)
lmF<-lm(Numeric[hunger$sex=="Female"]~Year[hunger$sex=="Female"], hunger)
lmF<-lm(Numeric[hunger$Sex=="Female"]~Year[hunger$Sex=="Female"], hunger)
lmM<-lm(Numeric[hunger$Sex=="Male"]~Year[hunger$Sex=="Male"], hunger)
lmBoth<-lm(Numeric~Year+Sex, hunger)
summary(lmBoth)$coef
summary(lmBoth)
lmInter<-lm(Numeric~Year+Sex+Year*Sex, hunger)
summary(lmInter)
fit<-lm(y ~ x, out2)
plot(fit, which=1)
fitno<-lm(y ~ x, out2[-1,]))
fitno<-lm(y ~ x, out2[-1,])
plot(fitno, which=1)
coef(fit)-coef(fitno)
head(dfbeta(fit))
resno<- out2[1, "y"] - predict(fitno, out2[1,])
1-resid(fit)[1]/resno
head(hatvalues(fit))
sigma<-sum(res(fit))/df(fit)
sigma<-sum(residuals(fit))/df(fit)
sigma<-(sum(residuals(fit))/df(fit))^2
residuals(fit)
sigma <- sqrt(deviance(fit)/df.residual(fit))
rstd<-resid(fit)/sigma*sqrt(1-hatvalues(fit)
)
rstd<-resid(fit)/(sigma*sqrt(1-hatvalues(fit)))
head(cbind(rstd, rstandard(fit)))
plot(fit, which=3)
plot(fit, which=2)
sigma1<-sqrt(deviance(fitno)/df.residual(fitno))
resid(fit)[1]/(sigma*sqrt(1-hatvalues(fit)[1]))
resid(fit)[1]/(sigma1*sqrt(1-hatvalues(fit)[1]))
head(rstudent(fit))
dy<-predict(fitno, out2)-predict(fit, out2)
sum(dy)^2/2*sigma^2
sum(dy^2)/(2*sigma^2)
plot(fit, which=5)
library(MASS)
?shuttle
library(MASS)
data(shuttle)
fit<-glm(use~wind, data=shuttle, family = "logistic")
summary(fit)
library(MASS)
data(shuttle)
fit<-glm(use~wind, data=shuttle, family = "binomial")
summary(fit)
library(MASS)
data(shuttle)
shuttle$use2<- as.numeric(shuttle$use)
fit<-glm(use2~wind, data=shuttle, family = "binomial")
summary(fit)
#QUizz 4
#Question 1
library(MASS)
data(shuttle)
shuttle$use2<- as.numeric(shuttle$use)
fit<-glm(use2~factor(wind), data=shuttle, family = "binomial")
summary(fit)
#QUizz 4
#Question 1
library(MASS)
data(shuttle)
shuttle$use2<- as.numeric(shuttle$use)
fit<-glm(use2~factor(wind)-1, data=shuttle, family = "binomial")
summary(fit)
#QUizz 4
#Question 1
library(MASS)
data(shuttle)
shuttle2<-shuttle
shuttle2$use2<- as.numeric(shuttle$use)
fit<-glm(use2~factor(wind)-1, data=shuttle2, family = "binomial")
summary(fit)
#QUizz 4
#Question 1
library(MASS)
data(shuttle)
shuttle2<-shuttle
shuttle2$use2<- as.numeric(shuttle2$use=='auto')
fit<-glm(use2~factor(wind) - 1, data=shuttle2, family = "binomial")
summary(fit)
exp(coef(fit))
exp(coef(fit)[1])/exp(coed(fit)[2])
exp(coef(fit)[1])/exp(coef(fit)[2])
fit2<-glm(use2 ~ factor(wind) + factor(magn) - 1, family = binomial, data = shuttle2)
summary(fit2)$coef
exp(coef(fit2))
exp(coef(fit2)[1])/exp(coef(fit2)[2])
data("InsectSprays")
fit3<-glm(count~factor(spray), data=InsectSprays, family = poisson)
summary(fit3)
summary(fit3)
coef(fit3)[1,1]/coef(fit3)[2,1]
coef(fit3)[1]/coef(fit3)[2]
summary(fit3)$coef
exp(coef(fit3))
exp(coef(fit3)[1])/exp(coef(fit3)[2])
data("InsectSprays")
fit3<-glm(count~factor(spray)-1, data=InsectSprays, family = poisson)
summary(fit3)$coef
exp(coef(fit3))
exp(coef(fit3)[1])/exp(coef(fit3)[2])
log(10)
library(swirl)
swirl()
x1c<-simbias()
apply(x1c, 1, mean)
fit1<-lm(Fertility~., swiss)
fit1 <- lm(Fertility ~ Agriculture, swiss)
fit3<-lm(Fertility~Agriculture+Examination+Education, swiss)
anova(fit1, fit3)
deviance(fit3)
d<-deviance(fit3)/43
deviance(fit1)-deviance(fit3)/(45-43)
deviance(fit1)-deviance(fit3)/2
n <- (deviance(fit1) - deviance(fit3))/2
n/d
pf(n/d, 2,43 lower.tail=FALSE)
pf(n/d, 2,43, lower.tail=FALSE)
shapiro.test(fit3$residuals)
anova(fit1, fit3, fit5, fit6)
ravenData
md1<-glm(ravenWinNum~ravenScore, data=ravenData, family=binomial)
mdl<-glm(ravenWinNum ~ ravenScore, family=binomial, data=ravenData)
lodds<-predict(mdl, data.frame(ravenScore=c(0, 3, 6)))
exp(lodds)/(1+exp(lodds))
Summary(mdl)
summary(mdl)
exp(confint(mdl))
anova(mdl)
qchisq(0.95, 1)
var(rpois(1000, 50))
head(hits)
class(hits[,'date'])
as.integer(head(hits[,'date']))
mdl <-
glm(visits ~ date, poisson, hits)
summary(mdl)
confint(mdl, 'date')
exp(confint(mdl, 'date'))
which.max(hits[,'visits'])
hits[704,]
lambda<-mdl$fitted.values[704]
qpois(.95, lambda)
mdl2<-glm(visits~date, poisson,)
mdl2<-glm(visits~date, poisson, hits ,offset=log(visits+1))
mdl2<-glm(simplystats~date, poisson, hits ,offset=log(visits+1))
qpois(.95, mdl2$fitted.values[704])
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
install.packages("caret")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(caret)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
install.packages("Hmisc")
head(training)
qplot(mixtures$CompressiveStrength)
plot(mixtures$CompressiveStrength)
plot(mixtures$CompressiveStrength, col=mixtures$Cement)
library(Hmisc)
mixtures$cementf<-cut2(mixtures$Cement)
plot(mixtures$CompressiveStrength, col=mixtures$cementf)
mixtures$BlastFurnaceSlagf<-cut2(mixtures$BlastFurnaceSlag)
plot(mixtures$CompressiveStrength, col=mixtures$BlastFurnaceSlagf)
mixtures$Agef<-cut2(mixtures$Age)
plot(mixtures$CompressiveStrength, col=mixtures$Agef)
mixtures$FlyAshf<-cut2(mixtures$FlyAsh)
plot(mixtures$CompressiveStrength, col=mixtures$FlyAshf)
corr(mixtures$FlyAsh, mixtures$CompressiveStrength)
cor(mixtures$FlyAsh, mixtures$CompressiveStrength)
cor(mixtures$Age, mixtures$CompressiveStrength)
hist(mixtures$Superplasticizer)
hist(log(mixtures$Superplasticizer)+1)
hist(log(mixtures$Superplasticizer))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
plot(mixtures$CompressiveStrength, col=mixtures$cementf)
names <- colnames(concrete)
names <- names[-length(names)]
featurePlot(x = training[, names], y = training$CompressiveStrength, plot = "pairs")
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
names <- colnames(concrete)
names <- names[-length(names)]
featurePlot(x = training[, names], y = training$CompressiveStrength, plot = "pairs")
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
featurePlot(x = training[, names], y = cutCS, plot = "box")
hist((mixtures$Superplasticizer))
head(training)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.8)
preProc$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.8)
preProc$rotation
hist(log(mixtures$Superplasticizer+1))
install.packages("corrplot")
setwd("D:/Documents/Data Science/Courses/Data Scientist Specialization (JOHN HOPKINS)/8. machine Learning")
---
output: html_document
---
#Quantified self movement
##Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.
In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to assess if they perform barbell lifts correctly and incorrectly (in 5 different ways).
## Libraries and Data Processing
```{r, echo=FALSE, results="hide", cache=TRUE}
library(caret)
library(corrplot)
library(kernlab)
library(randomForest)
```
Here we download the training and test set:
```{r, echo=FALSE, results="hide", cache=TRUE}
#get data urls
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#create directory if doesnot exist
if (!file.exists("data")){dir.create("data")}
#download data sets
download.file(url1, destfile=file.path("data","pml_training.csv"))
download.file(url2, destfile=file.path("data","pml_test.csv"))
```
#Cleaning Data
When looking at the training dataset, one can see a lot of missing data (NAs) in some columns. To increase our model efficiency we remove specific columns. In addition we are removing unusefull columns for the model (like the "name" column):
```{r, echo=FALSE, results="hide", cache=TRUE}
#read the data
train <- read.csv("data/pml_training.csv", na.strings= c("NA",""," "))
# cleaning the data
train_whithout_NAs <- apply(train, 2, function(x) {sum(is.na(x))})
train_cleaned <- train[,which(train_whithout_NAs == 0)]
# remove unusefull columns
train_cleaned <- train_cleaned[8:length(train_cleaned)]
```
# Create a model
We split the train set into a training set (70%) and a cross-validation set (30%):
```{r, echo=FALSE, results="hide", cache=TRUE}
inTrain <- createDataPartition(y = train_cleaned$classe, p = 0.7, list = FALSE)
t <- train_cleaned[inTrain, ]
cv <- train_cleaned[-inTrain, ]
```
As the problem is a classification problem, we can use a random forest model to fit the data:
```{r, echo=FALSE, results="hide", cache=TRUE}
model <- train(classe ~ ., data = t, method="rf")
?na.strings
---
output: html_document
---
#Quantified self movement
##Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.
In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to assess if they perform barbell lifts correctly and incorrectly (in 5 different ways).
## Libraries and Data Processing
```{r, echo=FALSE, results="hide", cache=FALSE}
library(caret)
library(corrplot)
library(kernlab)
library(randomForest)
```
Here we download the training and test set:
```{r, echo=FALSE, results="hide", cache=FALSE}
#get data urls
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#create directory if doesnot exist
if (!file.exists("data")){dir.create("data")}
#download data sets
download.file(url1, destfile=file.path("data","pml_training.csv"))
download.file(url2, destfile=file.path("data","pml_test.csv"))
```
#Cleaning Data
When looking at the training dataset, one can see a lot of missing data (NAs) in some columns. To increase our model efficiency we remove specific columns. In addition we are removing unusefull columns for the model (like the "name" column):
```{r, echo=FALSE, results="hide", cache=FALSE}
#read the data
train <- read.csv("data/pml_training.csv", na.strings= c("NA",""," "))
# cleaning the data
train_whithout_NAs <- apply(train, 2, function(x) {sum(is.na(x))})
train_cleaned <- train[,which(train_whithout_NAs == 0)]
# remove unusefull columns
train_cleaned <- train_cleaned[8:length(train_cleaned)]
```
# Create a model
We split the train set into a training set (70%) and a cross-validation set (30%):
```{r, echo=FALSE, results="hide", cache=FALSE}
inTrain <- createDataPartition(y = train_cleaned$classe, p = 0.7, list = FALSE)
t <- train_cleaned[inTrain, ]
cv <- train_cleaned[-inTrain, ]
```
As the problem is a classification problem, we can use a random forest model to fit the data:
```{r, echo=FALSE, results="hide", cache=FALSE}
model <- train(classe ~ ., data = t, method="rf")
library(caret)
library(corrplot)
library(kernlab)
library(randomForest)
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(./url1, destfile=file.path("data","pml_training.csv"))
download.file(url1, destfile=file.path("data","pml_training.csv"))
train <- read.csv("./data/pml_training.csv", na.strings= c("NA",""," "))
train_whithout_NAs <- apply(train, 2, function(x) {sum(is.na(x))})
train_cleaned <- train[,which(train_whithout_NAs == 0)]
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url2, destfile=file.path("data","pml_test.csv"))
test <- read.csv("./data/pml_test.csv", na.strings= c("NA",""," "))
---
output: html_document
---
#Quantified self movement
##Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.
In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to assess if they perform barbell lifts correctly and incorrectly (in 5 different ways).
## Libraries and Data Processing
```{r, echo=FALSE, results="hide", cache=FALSE}
library(caret)
library(corrplot)
library(kernlab)
library(randomForest)
```
Here we download the training and test set:
```{r, echo=FALSE, results="hide", cache=FALSE}
#get data urls
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#create directory if doesnot exist
if (!file.exists("data")){dir.create("data")}
#download data sets
download.file(url1, destfile=file.path("data","pml_training.csv"))
download.file(url2, destfile=file.path("data","pml_test.csv"))
```
#Cleaning Data
When looking at the training dataset, one can see a lot of missing data (NAs) in some columns. To increase our model efficiency we remove specific columns. In addition we are removing unusefull columns for the model (like the "name" column):
```{r, echo=FALSE, results="hide", cache=FALSE}
#read the data
train <- read.csv("./data/pml_training.csv", na.strings= c("NA",""," "))
# cleaning the data
train_whithout_NAs <- apply(train, 2, function(x) {sum(is.na(x))})
train_cleaned <- train[,which(train_whithout_NAs == 0)]
# remove unusefull columns
train_cleaned <- train_cleaned[8:length(train_cleaned)]
```
# Create a model
We split the train set into a training set (70%) and a cross-validation set (30%):
```{r, echo=FALSE, results="hide", cache=FALSE}
inTrain <- createDataPartition(y = train_cleaned$classe, p = 0.7, list = FALSE)
t <- train_cleaned[inTrain, ]
cv <- train_cleaned[-inTrain, ]
```
As the problem is a classification problem, we can use a random forest model to fit the data:
```{r, echo=FALSE, results="hide", cache=FALSE}
model <- train(classe ~ ., data = t, method="rf")
#Quantified self movement
##Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.
In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to assess if they perform barbell lifts correctly and incorrectly (in 5 different ways).
## Libraries and Data Processing
```{r, echo=FALSE, results="hide", cache=FALSE}
library(caret)
library(corrplot)
library(kernlab)
library(randomForest)
```
Here we download the training and test set:
```{r, echo=FALSE, results="hide", cache=FALSE}
#get data urls
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#create directory if doesnot exist
if (!file.exists("data")){dir.create("data")}
#download data sets
download.file(url1, destfile=file.path("data","pml_training.csv"))
download.file(url2, destfile=file.path("data","pml_test.csv"))
```
#Cleaning Data
When looking at the training dataset, one can see a lot of missing data (NAs) in some columns. To increase our model efficiency we remove specific columns. In addition we are removing unusefull columns for the model (like the "name" column):
```{r, echo=FALSE, results="hide", cache=FALSE}
#read the data
train <- read.csv("./data/pml_training.csv", na.strings= c("NA",""," "))
#get data urls
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#create directory if doesnot exist
if (!file.exists("data")){dir.create("data")}
#download data sets
download.file(url1, destfile=file.path("data","pml_training.csv"))
download.file(url2, destfile=file.path("data","pml_test.csv"))
```{r, echo=TRUE, results="hide", cache=FALSE}
#get data urls
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#create directory if doesnot exist
if (!file.exists("data")){dir.create("data")}
#download data sets
download.file(url1, destfile=file.path("data","pml_training.csv"))
download.file(url2, destfile=file.path("data","pml_test.csv"))
```
```{r, echo=FALSE, cache=TRUE}
#get data urls
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#create directory if doesnot exist
if (!file.exists("data")){dir.create("data")}
#download data sets
download.file(url1, destfile=file.path("data","pml_training.csv"))
download.file(url2, destfile=file.path("data","pml_test.csv"))
```
#read the data
train <- read.csv("./data/pml_training.csv", na.strings= c("NA",""," "))
# cleaning the data
train_whithout_NAs <- apply(train, 2, function(x) {sum(is.na(x))})
train_cleaned <- train[,which(train_whithout_NAs == 0)]
# remove unusefull columns
train_cleaned <- train_cleaned[8:length(train_cleaned)]
inTrain <- createDataPartition(y = train_cleaned$classe, p = 0.7, list = FALSE)
t <- train_cleaned[inTrain, ]
cv <- train_cleaned[-inTrain, ]
model <- train(classe ~ ., data = t, method="rf")
test <- read.csv("./data/pml_test.csv", na.strings= c("NA",""," "))
test_without_NAs <- apply(test, 2, function(x) {sum(is.na(x))})
test_cleaned <- test[,which(test_whitout_NAs == 0)]
test_cleaned <- test_cleaned[8:length(test_cleaned)]
test <- read.csv("./data/pml_test.csv", na.strings= c("NA",""," "))
test_without_NAs <- apply(test, 2, function(x) {sum(is.na(x))})
test_cleaned <- test[,which(test_without_NAs == 0)]
test_cleaned <- test_cleaned[8:length(test_cleaned)]
result2 <- predict(model, test_cleaned)
result2
